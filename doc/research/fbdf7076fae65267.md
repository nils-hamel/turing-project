## Generalisation analysis according to data-sets and hidden layer size of single layer auto-encoder

This study focus on the analysis of the generalisation and training perfromance
of auto-encoder according to the considered data-set and to the size of the
hidden layer of the auto-encoder. In this study, very simple single layer
auto-encoders are considered and the size of their unique hidden layer is modulated.

The implementation of the considered auto-encoder can be found [here](https://github.com/nils-hamel/turing-project/blob/master/src/turing-auto/auto-inv-hsv-osi-l2/README.md).

## Network training

This research considers a simple single-layer auto-encoder in order to analyse
its ability to generalise according to the considered data-set and the size of
its hidden layer.

For each data-set, the size of the hidden layer is modulated with the values :
64, 128, 256, 512, 1024 and 2048. The data-sets are split in two part : the
training part and the validation part. Usually, eighty percents of the data-set
is used for training. At each epoch, the objective function is exported for both
training and validation sub-sets.

## Training and validation loss

The following sub-sections shows the evolution of the training and validation
loss according to the considered data-sets and the size of the unique hidden
layer of the auto-encoder.

### 64x64x3-geneva-2009

The following plots shows the evolution of the training and validation loss for
the data-set [64x64x3-geneva-2009](https://github.com/nils-hamel/turing-project/blob/master/doc/dataset/64x64x3-geneva-2009.md).
The red curves stand for the training loss while the orange one represents the
evolution of the validation loss.

<p align="center">
    <img src="https://github.com/nils-hamel/turing-project/blob/master/doc/research/fbdf7076fae65267/64x64x3-geneva-2009-loss.jpg?raw=true" width="512">
    <br />
    <i>Training loss (red) and validation loss (orange) according to training epochs</i>
</p>
